<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!--
This is an example weekly progress report document that team members can use to report their individual progress 
of their ECE477 senior design projects. Weekly progress reports are expected to follow the general guidelines
presented in the "Progress Report Policy" document, available online at https://engineering.purdue.edu/ece477/Course/Policies/policies.html

Please create 4 copies of this example, renaming each copy to <PurdueID>.html, where <PurdueID> corresponds to
the Purdue ITAP Career Account ID given by Purdue to each individual team member. If you have any questions,
contact course staff.
-->
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

<!--Reconfigurable base tag; used to modify the site root location for root-relative links-->
<base href="https://engineering.purdue.edu/477grp12/" />

<!--Content-->
<title>ECE477 Course Documents</title>
<meta name="keywords" content="" />
<meta name="description" content="" />
<meta name="author" content="George Hadley">
<meta name = "format-detection" content = "telephone=no" />
<meta name="viewport" content="width=device-width,initial-scale=1.0">

<!--CSS-->
<link rel="stylesheet" href="css/default.css" type="text/css" media="all" />
<link rel="stylesheet" href="css/responsive.css">
<link rel="stylesheet" href="css/styles.css">
<link rel="stylesheet" href="css/content.css">
<!--[if IE 6]>
<link href="default_ie6.css" rel="stylesheet" type="text/css" />
<![endif]-->

</head>
<body>
<div id="wrapper_site">
    <div id="wrapper_page">
	<!-- Instantiate global site header.-->
	<div id="header"></div>
		<!-- Instantiate site global navigation bar.-->
		<div id="menu"></div>
	
		<!-- Instantiate a page banner image. Page banner images should be 1100x350px and should be located within the local
			img folder located at this directory level. -->
		<div id="banner">
			<img src="Files/img/BannerImgExample.jpg"></img>
		</div>
	
		<!-- Instantiate "tools" needed for a page. Tools are premade functional blocks that can be used to build a page,
			and include things such as a file lister (for listing out homework assignments or tutorials)
		-->
		<div id="content">
            <h2>Progress Report for Cole Giannotti</h2>
            
            <h4>Week 2:</h4>
            <b>Date:</b> 20 Jan 2017<br>
            <b>Total hours:</b> 4<br>
            <b>Description of design efforts:</b><br>
            The primary focus for the team this week was planning and preparation. I spent my time working on the Final Project Proposal and Functional Specification documents with the team. Being the team member with the digital signal processing experience, I had very specific research that was best suited for me for the Function Specification. One of our Project Specific Success Criteria is the ability to properly detect if the user is playing the correct note. This will allow us to provide feedback that is necessary for learning. I did research on how this detection would be done. The initial thought was to apply digital signal processing on the sound signal, by putting sensors on the individual strings, adding a microphone to pick up the notes, or reading the existing signal (due to the fact that we are working with electric and not acoustic guitars) that is sent to an amplifier. The first option of using Fast Fourier Transform (FFT) was put into question after Prof. Meyers stated that it is likely to run into problems using an FFT to do pitch recognition. After that conversation, I researched additional options including Autocorrelation and Zero-Cross Detection. I ran some simulations using MATLAB, as it is a good rapid prototyping environment. After viewing the results, it was clear that if a Zero-Cross Detection algorithm were used, the signal would have to be processed at the individual string level. Processing multiple frequencies would cause the sinusoidal components to cross the x-axis (crossing y = 0) in a different period than either of the signals individually, causing large amounts of aliasing.<br> 
            <div align="center">
            	<img style="image-orientation: 90deg;" width="30%" src="Team/progress/img/ColeImg/thumbnail_zerocross.png" class="rotate90"> 
            </div><br>
            The Digital Signal Processing concept was put on the back burner after a later conversation with the Lab Coordinator, George Hadley. He agreed with some of the worries I had about implementation of the pitch recognition system and offered an alternative that would stray away from Digital Signal Processing. The proposed idea is to detect contact with the guitar string and fret wires. This solution does not account for the sound of the guitar, meaning an out of tune guitar being played will inform the user that they are properly playing the song when in reality the motion of playing the song is correct while the sound being made is incorrect. I then worked with my team on ideas on how to detect the purposed contact. Pressure sensors, circuit closing, and capacitance sensors were all discussed collectively.
            </br>

            <br>

            <h4>Week 3:</h4>
            <b>Date:</b> 27 Jan 2017<br>
            <b>Total hours:</b> 9<br>
            <b>Description of design efforts:</b><br>
            After significant discussion with the ECE 477 course staff, the digital signal processing concept was officially revoked. With that said, we had the main hardware concepts layed out and removed a large level of complexityfrom the software requirements. Brian and I took the opportunity to lay out as much of the software as possible. The first issue to tackle was data management and encoding. I spent some time brainstorming ideas to present to the team. There were three main contenders for data encoding. The first was splitting the bits of one byte into two groups and assigning each group to designate either the string or fret respectively. This works perfectly because a 3/5 split on an 8 bit set allows for a comprehensive set of options for both strings and frets with zero wasted bits. The possible combinations were not exhaustive of the available range in a single byte which allows for specialty characters to be stored. The second option was an array of characters where each character would represent the fret for its given string based on position within the array. This method is very easy to read to the human eye but is not effecient on space saving or time to read. The final option was to create a binary search tree encoding for each string-fret combination and store songs as keys to the tree. This method would be very effecient to search for the data, however we found that it was redundant compared to the first idea because string and fret information would still have to be stored. Storing the string and fret information within each leaf would be most effeciently done by creating a 3/5 split on a single byte, which is identical to what was already proposed. A combination of the first and second option is what was decided by the team, with Brian and I having frequent discussions on the matter.
            <br>
            <div align="center">
            	<img style="image-orientation: 90deg;" width="30%" src="Team/progress/img/ColeImg/StateMachineDiagram.jpg"> 
            </div><br>
            With the deadline of the software overview this week. I spent a lot of time building the framework for what the software and firmware would be doing. In order to better formulate my thoughts, I made a series of flowcharts that depicted various steps of code's process. After discussing the flowcharts with Brian, he developed a good state-machine diagram to even further convey the algorithms. I then created a formal version of his diagram and included it within the software overview. On top of all the tangable progress, I have put a great deal of thought on how the interfaces of the remote user controller should be set up. Were we to design a phone application, I have brainstormed some concepts of how it would be designed.
            </br>

            <br>
	    <h4>Week 4:</h4>
            <b>Date:</b>03 Feb 2017<br>
            <b>Total hours:</b> 8<br>
            <b>Description of design efforts:</b><br>
            The team spent the week making some of the final design choices and preperation work before we start to "get our hands dirty" in the upcoming weeks. We put in a lot of thought and brainstorming into methods to reduce the number of wires needed to run up the neck of the guitar to properly power everything. After the always wonderful help of the course staff we decided on a mirrored system for LEDs and fret pressing recognition. The concept is to make a switch matrix that acts as a large grid where we can index signals to positions based off of their row and column. These signals will be cycled at a high frequency to make the illusion that multiple LEDs and strings are being powered at once to the human eye. However in reality only one LED or string will be powered at any given time but the rate at which they change will be quicker than humans can detect. In order to protect the circuitry we have decided on exposing the integrated circuitry to the user visually, but covering the electronic with a layer of clear plastic.
            <br>
            <div align="center">
            	<img style="image-orientation: 90deg;" width="30%" src="Team/progress/img/ColeImg/LiquidRubber.jpeg"> 
            </div><br>
            With physical construction of Guitutar approaching quickly, we needed to start getting parts. Due to where I live in comparison to my teammates, it made the most sense for me to make a trip to get supplies. I headed to Menards to get liquid rubber and extra guitar strings to test our ability to isolate signals inside the guitar strings as they enter the metal frame inside the body of the guitar. After finding the liquid rubber but being unable to find guitar strings, I developed a circuit that could properly prove that the liquid rubber could isolate the strings inside the guitar.
            </br>


	    <br>	    
	    <h4>Week 5:</h4>
            <b>Date:</b>10 Feb 2017<br>
            <b>Total hours:</b> 5<br>
            <b>Description of design efforts:</b><br>
            With physical parts in shipment, my time this week was spent preparing the software for the phone application. The team discussed whether to make an android app or an iPhone app. The majority of the team (3 out of 4) currently own iPhones while only one of us owns an android phone. This was a negative for an android solution; however, a positive is that development for androids has a lower barrier of entry with the tools given through the android development studio. After some consideration, we realized that an android app would be better because its main problem can be mitigated with the use of emulators (which comes standard with android studio). Emulation is a step above simulation. Where simulation replicates a similar environment to the actual environment, emulation replicates the exact environment. With android studioâ€™s device emulation, we can run and test the phone app like if we had an actual android phone ourselves. Instead of having to buy an expensive smart phone to test our program, a virtual machine will be made on our computers. For every hardware specification that a specific android phone would have, the emulator would replicate the exact same interactions.
            <br>
            <div align="center">
            	<img style="image-orientation: 90deg;" width="100%" src="Team/progress/img/ColeImg/GuitutarHelloWorld.png"> 
            </div><br>
            The android studio integrated development environment should be easy to for me to pick up because it allows for coding to be done in Java. Java is an object-oriented coding language that I have become familiar with from previous courses in ECE. An object-oriented language means that the code is structured around the concept of structuring data to model a real-life object, and then customizing each instance of an object to make it unique. This will be applied to the phone app to have an object that controls how pages on the app should interact with other objects, while making multiple instances of these pages so that each page as a unique instance controlling it.
            </br>

<br>	    
	    <h4>Week 6:</h4>
            <b>Date:</b>17 Feb 2017<br>
            <b>Total hours:</b> 5<br>
            <b>Description of design efforts:</b><br>
            This week I spent more time working on the phone app and getting used to android studio development. In order to prepare for full on navigation development, I started with understanding button presses that would be used for navigation. I used a built in widget called "Toast" to help display messages when buttons were pressed. The purpose of the messages were to verify at runtime that when I press a button, I can make certain actions happen which I could easily notice with pop-up messages. The Toast messages were executed by lines in the format of [Toast.makeText(getApplicationContext(), "Message to display", Toast.LENGTH).show();]. The makeText method of Toast uses getApplicationContext() to get the current screen that the phone is displaying inorder to know where to print a message to. It then takes in a message to display and the size of the message box, either LENGTH_LONG or LENGTH_SHORT which are constants defined within the Toast class. Finally when the makeText method is finished making a message box, the show method is executed on it to get pushed onto the phone's screen. Without the show method, multiple text boxes could be made and stored as variables but never shown, or perhaps saved to show at a later time.
            <br>
            <div align="center">
            	<img style="image-orientation: 90deg;" width="75%" src="Team/progress/img/ColeImg/ToastButtons.png"> 
            </div><br>
            The button presses use a listener class to know when a button has actually been pressed. A listener class is code that simply "listens" for an event to occur in order to run specific functions or methods. It can be comparable to a state machine that uses flags being set in order to know what code needs to be executed at any given time. On creation of the application's main class, the constructor (code responsible for making an instance of a defined class) calls the onCreate method. The onCreate method then has this line, [navigationView.setNavigationItemSelectedListener(this);]. This line tells the navigation window that the "this" object (which is the instance currently being created) should be notified anytime a navigation item is selected from which the listener becomes aware of. When the listener noticed that a navigation item was selected, it calls the onNavigationItemSelected(MenuItem) method which passes the information of which menu item in the navigation window was selected to the current instance of the application. The application then can do with the information as it pleases. In my case, this is when I utilized the Toast messages to display a quick message to notify the user that they have successfully selected the button that they just clicked.
            </br>

            <br>
            
	    <h4>Week 7:</h4>
            <b>Date:</b>24 Feb 2017<br>
            <b>Total hours:</b> 8<br>
            <b>Description of design efforts:</b><br>
            This week the team and I removed the fretboard from a guitar neck that we are using as a test subject. The fretboard is mounted onto the neck of the guitar soley by powerful woodglue. In order to remove the fretboard, the glue had to be weakened substantially. Through hours of research, we noticed the most often method used to weaken the glue was to use a clothing iron to heat and disolve the glue via steam. I brought an iron and towel to our lab and we started work on removing the fretboard. We made some progress after an hour, but it seemed slow. The iron was having an issue laying flush with the fretboard therefore the heat transfer had high amounts of dissipation before arriving at the glue. We dug deeper into research and came upon the idea to use a heat gun to get the glue to high temperatures. The heat gun made with the assumption that contact will not be made with the material that is desired to be heated. This solved our problem of the iron not laying flush, as the iron's design was made assuming that any material it was heating would be laid flush against its surface (ei clothes). The heat gun also was able to go to MUCH higher temperatures than the iron. The iron maxed out around 400 degrees fahrenheit, while the heat gun was able to surpass 1000 degrees fahrenheit. We set the heat gun to 750 degrees fahrenheit and continued on our way. We instantly noticed a large imporvement on the speed of our process and were able to do double the removal in half the time compared to the iron. When we finished removing the fretboard, there were artifacts of the two processes and their effeciencies in the form of how it left the wood. The places were we had used the iron were looked damaged and had traces of left over glue because it was not heated all the way. On the other hand the places where the heat gun was used, the wood looked smooth and finished as if it had almost never been glued at all.
            <br>
            <div align="center">
            	<img style="image-orientation: 90deg;" width="75%" src="Team/progress/img/BrianImg/RemovedFretboard.jpg"> 
            </div><br>
            Secondly, the software formalization was due this week for the group. Brian and I are mostly responsible for the software components of the project, so I worked closely with him on finalizing our software design and definitions. We met to discuss topics like song endcoding/decoding, device commutication, storage design, and more. To go in depth on an example, say storage design, we worked on where and how we wanted to store all the information. The first issue was storing all songs. This was an obvious choice as we know a phone's flash memory tends to be fairly large, so we decided to pull from there in order to reduce requirements on our hardware design. Second we discussed storage of the active song. We had two ideas. One idea was we would have the phone send note data to the microcontroller note by note in real time, only storing only a four to 10 bytes at a time within the microcontroller at a time. The other idea was to transfer an entire song's information to the microcontroller before the song could be played as a psuedo download/install mechanic. The primary benefit of the first method was reduced storage, where its downside was increased requirement on quick and uninterrupted communication. The second method's pros were that real time playing of songs was not reliant on continuous communication between the devices, however its cons were increased storage requirements. After some discussion, we decided that storage would never be an issue with the given memory on the PIC microcontroller already. With the only con on the second method's list removed, the loading method seemed to be the better option.
            </br>

            <br>
                      
        </div>
	
		<!-- Instantiate global footer. Any changes to the footer should be made through the top-level file "footer.html" -->
		<div id="footer"></div>
    </div>
</div>

<!--JS-->
<script src="js/jquery.js"></script>
<script src="js/jquery-migrate-1.1.1.js"></script>

<script type="text/javascript">
$(document).ready(function() {
    $("#header").load("header.html");
	$("#menu").load("navbar.html");
	$("#footer").load("footer.html");
});
</script>
</body>
</html>
