<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!--
This is an example weekly progress report document that team members can use to report their individual progress 
of their ECE477 senior design projects. Weekly progress reports are expected to follow the general guidelines
presented in the "Progress Report Policy" document, available online at https://engineering.purdue.edu/ece477/Course/Policies/policies.html

Please create 4 copies of this example, renaming each copy to <PurdueID>.html, where <PurdueID> corresponds to
the Purdue ITAP Career Account ID given by Purdue to each individual team member. If you have any questions,
contact course staff.
-->
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

<!--Reconfigurable base tag; used to modify the site root location for root-relative links-->
<base href="https://engineering.purdue.edu/477grp12/" />

<!--Content-->
<title>ECE477 Course Documents</title>
<meta name="keywords" content="" />
<meta name="description" content="" />
<meta name="author" content="George Hadley">
<meta name = "format-detection" content = "telephone=no" />
<meta name="viewport" content="width=device-width,initial-scale=1.0">

<!--CSS-->
<link rel="stylesheet" href="css/default.css" type="text/css" media="all" />
<link rel="stylesheet" href="css/responsive.css">
<link rel="stylesheet" href="css/styles.css">
<link rel="stylesheet" href="css/content.css">
<!--[if IE 6]>
<link href="default_ie6.css" rel="stylesheet" type="text/css" />
<![endif]-->

</head>
<body>
<div id="wrapper_site">
    <div id="wrapper_page">
	<!-- Instantiate global site header.-->
	<div id="header"></div>
		<!-- Instantiate site global navigation bar.-->
		<div id="menu"></div>
	
		<!-- Instantiate a page banner image. Page banner images should be 1100x350px and should be located within the local
			img folder located at this directory level. -->
		<div id="banner">
			<img src="Files/img/BannerImgExample.jpg"></img>
		</div>
	
		<!-- Instantiate "tools" needed for a page. Tools are premade functional blocks that can be used to build a page,
			and include things such as a file lister (for listing out homework assignments or tutorials)
		-->
		<div id="content">
            <h2>Progress Report for Cole Giannotti</h2>
            
            <h4>Week 2:</h4>
            <b>Date:</b> 20 Jan 2017<br>
            <b>Total hours:</b> 4<br>
            <b>Description of design efforts:</b><br>
            The primary focus for the team this week was planning and preparation. I spent my time working on the Final Project Proposal and Functional Specification documents with the team. Being the team member with the digital signal processing experience, I had very specific research that was best suited for me for the Function Specification. One of our Project Specific Success Criteria is the ability to properly detect if the user is playing the correct note. This will allow us to provide feedback that is necessary for learning. I did research on how this detection would be done. The initial thought was to apply digital signal processing on the sound signal, by putting sensors on the individual strings, adding a microphone to pick up the notes, or reading the existing signal (due to the fact that we are working with electric and not acoustic guitars) that is sent to an amplifier. The first option of using Fast Fourier Transform (FFT) was put into question after Prof. Meyers stated that it is likely to run into problems using an FFT to do pitch recognition. After that conversation, I researched additional options including Autocorrelation and Zero-Cross Detection. I ran some simulations using MATLAB, as it is a good rapid prototyping environment. After viewing the results, it was clear that if a Zero-Cross Detection algorithm were used, the signal would have to be processed at the individual string level. Processing multiple frequencies would cause the sinusoidal components to cross the x-axis (crossing y = 0) in a different period than either of the signals individually, causing large amounts of aliasing.<br> 
            <div align="center">
            	<img style="image-orientation: 90deg;" width="30%" src="Team/progress/img/ColeImg/thumbnail_zerocross.png" class="rotate90"> 
            </div><br>
            The Digital Signal Processing concept was put on the back burner after a later conversation with the Lab Coordinator, George Hadley. He agreed with some of the worries I had about implementation of the pitch recognition system and offered an alternative that would stray away from Digital Signal Processing. The proposed idea is to detect contact with the guitar string and fret wires. This solution does not account for the sound of the guitar, meaning an out of tune guitar being played will inform the user that they are properly playing the song when in reality the motion of playing the song is correct while the sound being made is incorrect. I then worked with my team on ideas on how to detect the purposed contact. Pressure sensors, circuit closing, and capacitance sensors were all discussed collectively.
            </br>

            <br>

            <h4>Week 3:</h4>
            <b>Date:</b> 27 Jan 2017<br>
            <b>Total hours:</b> 9<br>
            <b>Description of design efforts:</b><br>
            After significant discussion with the ECE 477 course staff, the digital signal processing concept was officially revoked. With that said, we had the main hardware concepts layed out and removed a large level of complexityfrom the software requirements. Brian and I took the opportunity to lay out as much of the software as possible. The first issue to tackle was data management and encoding. I spent some time brainstorming ideas to present to the team. There were three main contenders for data encoding. The first was splitting the bits of one byte into two groups and assigning each group to designate either the string or fret respectively. This works perfectly because a 3/5 split on an 8 bit set allows for a comprehensive set of options for both strings and frets with zero wasted bits. The possible combinations were not exhaustive of the available range in a single byte which allows for specialty characters to be stored. The second option was an array of characters where each character would represent the fret for its given string based on position within the array. This method is very easy to read to the human eye but is not effecient on space saving or time to read. The final option was to create a binary search tree encoding for each string-fret combination and store songs as keys to the tree. This method would be very effecient to search for the data, however we found that it was redundant compared to the first idea because string and fret information would still have to be stored. Storing the string and fret information within each leaf would be most effeciently done by creating a 3/5 split on a single byte, which is identical to what was already proposed. A combination of the first and second option is what was decided by the team, with Brian and I having frequent discussions on the matter.
            <br>
            <div align="center">
            	<img style="image-orientation: 90deg;" width="30%" src="Team/progress/img/ColeImg/StateMachineDiagram.jpg"> 
            </div><br>
            With the deadline of the software overview this week. I spent a lot of time building the framework for what the software and firmware would be doing. In order to better formulate my thoughts, I made a series of flowcharts that depicted various steps of code's process. After discussing the flowcharts with Brian, he developed a good state-machine diagram to even further convey the algorithms. I then created a formal version of his diagram and included it within the software overview. On top of all the tangable progress, I have put a great deal of thought on how the interfaces of the remote user controller should be set up. Were we to design a phone application, I have brainstormed some concepts of how it would be designed.
            </br>

            <br>
	    <h4>Week 4:</h4>
            <b>Date:</b>03 Feb 2017<br>
            <b>Total hours:</b> 8<br>
            <b>Description of design efforts:</b><br>
            The team spent the week making some of the final design choices and preperation work before we start to "get our hands dirty" in the upcoming weeks. We put in a lot of thought and brainstorming into methods to reduce the number of wires needed to run up the neck of the guitar to properly power everything. After the always wonderful help of the course staff we decided on a mirrored system for LEDs and fret pressing recognition. The concept is to make a switch matrix that acts as a large grid where we can index signals to positions based off of their row and column. These signals will be cycled at a high frequency to make the illusion that multiple LEDs and strings are being powered at once to the human eye. However in reality only one LED or string will be powered at any given time but the rate at which they change will be quicker than humans can detect. In order to protect the circuitry we have decided on exposing the integrated circuitry to the user visually, but covering the electronic with a layer of clear plastic.
            <br>
            <div align="center">
            	<img style="image-orientation: 90deg;" width="30%" src="Team/progress/img/ColeImg/LiquidRubber.jpeg"> 
            </div><br>
            With physical construction of Guitutar approaching quickly, we needed to start getting parts. Due to where I live in comparison to my teammates, it made the most sense for me to make a trip to get supplies. I headed to Menards to get liquid rubber and extra guitar strings to test our ability to isolate signals inside the guitar strings as they enter the metal frame inside the body of the guitar. After finding the liquid rubber but being unable to find guitar strings, I developed a circuit that could properly prove that the liquid rubber could isolate the strings inside the guitar.
            </br>


	    <br>	    
	    <h4>Week 5:</h4>
            <b>Date:</b>10 Feb 2017<br>
            <b>Total hours:</b> 5<br>
            <b>Description of design efforts:</b><br>
            With physical parts in shipment, my time this week was spent preparing the software for the phone application. The team discussed whether to make an android app or an iPhone app. The majority of the team (3 out of 4) currently own iPhones while only one of us owns an android phone. This was a negative for an android solution; however, a positive is that development for androids has a lower barrier of entry with the tools given through the android development studio. After some consideration, we realized that an android app would be better because its main problem can be mitigated with the use of emulators (which comes standard with android studio). Emulation is a step above simulation. Where simulation replicates a similar environment to the actual environment, emulation replicates the exact environment. With android studio’s device emulation, we can run and test the phone app like if we had an actual android phone ourselves. Instead of having to buy an expensive smart phone to test our program, a virtual machine will be made on our computers. For every hardware specification that a specific android phone would have, the emulator would replicate the exact same interactions.
            <br>
            <div align="center">
            	<img style="image-orientation: 90deg;" width="100%" src="Team/progress/img/ColeImg/GuitutarHelloWorld.png"> 
            </div><br>
            The android studio integrated development environment should be easy to for me to pick up because it allows for coding to be done in Java. Java is an object-oriented coding language that I have become familiar with from previous courses in ECE. An object-oriented language means that the code is structured around the concept of structuring data to model a real-life object, and then customizing each instance of an object to make it unique. This will be applied to the phone app to have an object that controls how pages on the app should interact with other objects, while making multiple instances of these pages so that each page as a unique instance controlling it.
            </br>

<br>	    
	    <h4>Week 6:</h4>
            <b>Date:</b>17 Feb 2017<br>
            <b>Total hours:</b> 5<br>
            <b>Description of design efforts:</b><br>
            This week I spent more time working on the phone app and getting used to android studio development. In order to prepare for full on navigation development, I started with understanding button presses that would be used for navigation. I used a built in widget called "Toast" to help display messages when buttons were pressed. The purpose of the messages were to verify at runtime that when I press a button, I can make certain actions happen which I could easily notice with pop-up messages. The Toast messages were executed by lines in the format of [Toast.makeText(getApplicationContext(), "Message to display", Toast.LENGTH).show();]. The makeText method of Toast uses getApplicationContext() to get the current screen that the phone is displaying inorder to know where to print a message to. It then takes in a message to display and the size of the message box, either LENGTH_LONG or LENGTH_SHORT which are constants defined within the Toast class. Finally when the makeText method is finished making a message box, the show method is executed on it to get pushed onto the phone's screen. Without the show method, multiple text boxes could be made and stored as variables but never shown, or perhaps saved to show at a later time.
            <br>
            <div align="center">
            	<img style="image-orientation: 90deg;" width="75%" src="Team/progress/img/ColeImg/ToastButtons.png"> 
            </div><br>
            The button presses use a listener class to know when a button has actually been pressed. A listener class is code that simply "listens" for an event to occur in order to run specific functions or methods. It can be comparable to a state machine that uses flags being set in order to know what code needs to be executed at any given time. On creation of the application's main class, the constructor (code responsible for making an instance of a defined class) calls the onCreate method. The onCreate method then has this line, [navigationView.setNavigationItemSelectedListener(this);]. This line tells the navigation window that the "this" object (which is the instance currently being created) should be notified anytime a navigation item is selected from which the listener becomes aware of. When the listener noticed that a navigation item was selected, it calls the onNavigationItemSelected(MenuItem) method which passes the information of which menu item in the navigation window was selected to the current instance of the application. The application then can do with the information as it pleases. In my case, this is when I utilized the Toast messages to display a quick message to notify the user that they have successfully selected the button that they just clicked.
            </br>

            <br>
            
	    <h4>Week 7:</h4>
            <b>Date:</b>24 Feb 2017<br>
            <b>Total hours:</b> 8<br>
            <b>Description of design efforts:</b><br>
            This week the team and I removed the fretboard from a guitar neck that we are using as a test subject. The fretboard is mounted onto the neck of the guitar soley by powerful woodglue. In order to remove the fretboard, the glue had to be weakened substantially. Through hours of research, we noticed the most often method used to weaken the glue was to use a clothing iron to heat and disolve the glue via steam. I brought an iron and towel to our lab and we started work on removing the fretboard. We made some progress after an hour, but it seemed slow. The iron was having an issue laying flush with the fretboard therefore the heat transfer had high amounts of dissipation before arriving at the glue. We dug deeper into research and came upon the idea to use a heat gun to get the glue to high temperatures. The heat gun made with the assumption that contact will not be made with the material that is desired to be heated. This solved our problem of the iron not laying flush, as the iron's design was made assuming that any material it was heating would be laid flush against its surface (ei clothes). The heat gun also was able to go to MUCH higher temperatures than the iron. The iron maxed out around 400 degrees fahrenheit, while the heat gun was able to surpass 1000 degrees fahrenheit. We set the heat gun to 750 degrees fahrenheit and continued on our way. We instantly noticed a large imporvement on the speed of our process and were able to do double the removal in half the time compared to the iron. When we finished removing the fretboard, there were artifacts of the two processes and their effeciencies in the form of how it left the wood. The places were we had used the iron were looked damaged and had traces of left over glue because it was not heated all the way. On the other hand the places where the heat gun was used, the wood looked smooth and finished as if it had almost never been glued at all.
            <br>
            <div align="center">
            	<img style="image-orientation: 90deg;" width="75%" src="Team/progress/img/BrianImg/7_RemovedFretboard.jpg"> 
            </div><br>
            Secondly, the software formalization was due this week for the group. Brian and I are mostly responsible for the software components of the project, so I worked closely with him on finalizing our software design and definitions. We met to discuss topics like song endcoding/decoding, device commutication, storage design, and more. To go in depth on an example, say storage design, we worked on where and how we wanted to store all the information. The first issue was storing all songs. This was an obvious choice as we know a phone's flash memory tends to be fairly large, so we decided to pull from there in order to reduce requirements on our hardware design. Second we discussed storage of the active song. We had two ideas. One idea was we would have the phone send note data to the microcontroller note by note in real time, only storing only a four to 10 bytes at a time within the microcontroller at a time. The other idea was to transfer an entire song's information to the microcontroller before the song could be played as a psuedo download/install mechanic. The primary benefit of the first method was reduced storage, where its downside was increased requirement on quick and uninterrupted communication. The second method's pros were that real time playing of songs was not reliant on continuous communication between the devices, however its cons were increased storage requirements. After some discussion, we decided that storage would never be an issue with the given memory on the PIC microcontroller already. With the only con on the second method's list removed, the loading method seemed to be the better option.
            </br>

            <br>

	    <h4>Week 8:</h4>
            <b>Date:</b>10 Mar 2017<br>
            <b>Total hours:</b> 10<br>
            <b>Description of design efforts:</b><br>
            After just finishing the midterm design review and spring break coming up, the team split up work to make adjustments from our evaluation feedback. Being the one that is mainly responsible for the phone app, I got back to working on more practical and technical components for it. To remove the requirement of the microcontroller having to store data in flash memory, we are going to use the phone's harddrive to store all song information to then be transfered to the microcontroller at time. In order to do this, two important pieces are required. One is a database in which to store the song information, and two is Bluetooth communication to transmit the data of a song. This week I worked on the former and have set up a database in which the songs can be stored on the phone. The database will allow for all the information like song name, tempo, notes, and highscores to be saved and retrieved as a whole. This will help promote effeciency when isolating the data for one individual song compared to methods like saving the data within a file that is not controlled by a structural scheme.
            <br>
            <div align="center">
            	<img style="image-orientation: 90deg;" width="75%" src="Team/progress/img/ColeImg/DatabaseContract.png"> 
            </div><br>
            <br>


	    <h4>Week 10:</h4>
            <b>Date:</b>31 Mar 2017<br>
            <b>Total hours:</b> 9<br>
            <b>Description of design efforts:</b><br>
            With hardware prototyping coming to a closure, I set out to work on Bluetooth connection this week. The phone app will communicate to te microcontroller via Bluetooth to transmit data wirelessly. Android Studio has built in Bluetooth API so I took the time to familiarize myself with it and write some test code to be used on our initial pairing of the phone and microcontroller. The Bluetooth API works in that a Bluetooth reciever can scan for other Bluetooth devices within its proximity. After two devices have made communication, a pairing of them will be requested as a sort of secuirty clearence and paperwork. These pairings can then be accessed at a later time to communicate with devices without having to set up connection again. Below is some sample code that will be used to detect the Bluetooth module in the microcontroller to properly set up communication when hardware demoing occurs.
            <br>
            <div align="center">
            	<img style="image-orientation: 90deg;" width="75%" src="Team/progress/img/ColeImg/BluetoothScanning.png"> 
            </div><br>
            <br>

	    <h4>Week 13:</h4>
            <b>Date:</b>07 April 2017<br>
            <b>Total hours:</b> 12<br>
            <b>Description of design efforts:</b><br>
            This week I spent some time working on the microcontroller code and the phone app code. Brian and I worked together on the microcontroller to make the necessary functions to quickly decode note information to light up the LEDs on the neck of the guitar. In order to test our design we programmed some test notes to be displayed in progression. To simulate the effect of seeing the user play the note properly, we added a push button that signaled to move on to the next note. We were successfully able to display unique notes and iterate through them via push button. This now only needs to be applied to incoming song data instead of hard-coded notes in order to globally work amongst all LED strips in the frets. 
            <br>
            <div align="center">
            	<img style="image-orientation: 90deg;" width="75%" src="Team/progress/img/ColeImg/AndroidCreateSong.png"> 
            </div><br>
	    I also worked a function on the phone app that allows users to make their own songs by specifying sequences of string and fret combinations. This not only will be useful for real life use, but also will serve as a way for us to quickly create so encodings without having to a large amount of binary to decimal conversations manually. 
            <br>

	    <h4>Week 14:</h4>
            <b>Date:</b>14 April 2017<br>
            <b>Total hours:</b> 6<br>
            <b>Description of design efforts:</b><br>
            This week Brian and I worked and Bluetooth some more because it is crucial to our project specific success criteria.  We sifted through the harmony library trying to find the proper channels to program the Bluetooth module, but the built in functionality is not compatible with our device. We then moved on track to figure out how to test our device, but it will require a breakout board for quick prototyping. 
            <br>
            <div align="center">
            	<img style="image-orientation: 90deg;" width="75%" src="Team/progress/img/ColeImg/BluetoothWrite.png"> 
            </div><br>
	    I then set out to improve upon the app's Bluetooth code as we prepare for testing of the guitar side device. I did a great deal of reading about server and client connections between devices. The server and client devices connect to each other through what is called a Bluetooth socket. The server device opens up a socket as sort of a "docking station" while the client consumes the socket to open a line of communication. At that point, Android's built in BluetoothSocket, InputStream, and OutputStream classes have a majority of the necessary communication functions built in. I took the time to familiarize myself with the classes and have general code in place for testing once our Bluetooth hardware is up and running. 
            <br>

 	    <h4>Week 15:</h4>
            <b>Date:</b>21 April 2017<br>
            <b>Total hours:</b> 8<br>
            <b>Description of design efforts:</b><br>
            After receiving all the finalized components that we ordered, we got right to soldering our PCB. Because my main focus has been on the software side, I soldered on our surface mount LEDs in order to test and demo the microcontroller's ability to display note information as a binary matrix. It was the first time I had soldered a surface mounted component onto a PCB, so I enlisted the help of Matt, my team's adviser. He walked me through the common techniques of applying solder first then placing the component and the opposite in which the component is placed first then pre-melted solder is applied. I took the route of placing the component first. It was a lot trickier than I had anticipated but after some practice I got into a rhythm and it felt comfortable. We were able to light the LEDs on the board after they were on the board. 
            <br>
            <div align="center">
            	<img style="image-orientation: 90deg;" width="75%" src="Team/progress/img/ColeImg/PCB.jpeg"> 
            </div><br>
	    This week the group was held back by some debugging issues that consumed a large portion of our time. The microcontroller had a slew of errors when trying to program it on the PCB. From powering issues to the inability to recognize the device, programming the microcontroller was not working. After every test we ran came back inconclusive, we decided to start fresh with a new board and new microcontroller to see if the issue was localized in those hardware components specifically. Joe helped us solder the new microcontroller on the new board quickly to help us reduce downtime. When the new hardware was finished, we were able to program the microcontroller after a small amount of fiddling with the wires. We adjourned there with the anticipation of making more substantial progress this weekend. 
            <br>

	    <h4>Week 16:</h4>
            <b>Date:</b>28 April 2017<br>
            <b>Total hours:</b> 40<br>
            <b>Description of design efforts:</b><br>
            I don't have time to explain why I don't have time to explain everything I did this past week (Destiny reference). This week was a frantic dash to try to get everything done on time. After spending countless hours understanding our Bluetooth module, we ended up scrapping that device for the RN4020. Then after spending some more time learning how to talk to the RN4020 we decided we needed to cut our Bluetooth entirely for the spark challenge. So then Brian and I got to cleaning up the microcontroller code. There were some issues where if the low E string had an LED lit and then another fret was getting a light up signal, then the whole row of the new fret would light up. We band-aided this by just changing the order in which we told LEDs to light up, but in order to fix things entirely we set out to restructure the data structures uses and effective "classes" (no real classes were used because the micro was being coded in a non-OOP language - C). We couldn't test the new code intermediately because Austin and Jen were chugging away trying to get all the PCBs connected. So once we had all the PCBs connected and tried our new and improved code, it didn't light up. Unfortunately for us we were unable to get any debugging interface to work with our microcontroller so we ended up having to scrap the new and "improved" code in order to have a working product for spark challenge because there was to much to test without any good ways of testing it. Something that is only somewhat a sad story this week is I wrote a rolling averager data structure that would take a rolling average of a digital input pin's status over the course of some time in order to smooth out our strum detection. This was important because the strum detetction was detecting vibration so there would be many high peaks in a given time frame that in sum indicated a strum. It was written to be as time effecient as possible in order to not slow down the user's interaction with the system. I don't have any pictures because taking pictures wasn't that high on my priority list this week. Sorry for the bluntness, it's almost midnight and I didn't sleep at all last night... I am ready for bed. God Bless America.
            <br>
            <div align="center">
            	<img style="image-orientation: 90deg;" width="75%" src="Team/progress/img/AustinImg/guitutar_LED.png"> 
            </div><br>          
        </div>
	
		<!-- Instantiate global footer. Any changes to the footer should be made through the top-level file "footer.html" -->
		<div id="footer"></div>
    </div>
</div>

<!--JS-->
<script src="js/jquery.js"></script>
<script src="js/jquery-migrate-1.1.1.js"></script>

<script type="text/javascript">
$(document).ready(function() {
    $("#header").load("header.html");
	$("#menu").load("navbar.html");
	$("#footer").load("footer.html");
});
</script>
</body>
</html>
